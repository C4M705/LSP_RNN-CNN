{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VuI3Z8l1Ps8a"
   },
   "source": [
    "\n",
    "# **<center><font style=\"color:rgb(100,109,254)\">Entrenamiento de modelo (CNN + LSTM)</font> </center>**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RiAMJx7tr5-I"
   },
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\"> Importar librerías</font>**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ft-u2b1iUQpg"
   },
   "outputs": [],
   "source": [
    "# Discard the output of this cell.\n",
    "#%%capture\n",
    "\n",
    "# Install the required libraries.\n",
    "!pip install tensorflow opencv-contrib-python youtube-dl moviepy pydot graphviz\n",
    "!pip install git+https://github.com/TahaAnwar/pafy.git#egg=pafy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "X3AdbpZFCRR0"
   },
   "outputs": [],
   "source": [
    "# Import the required libraries.\n",
    "import os\n",
    "import cv2\n",
    "import pafy\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "\n",
    "from moviepy.editor import *\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxRZOCSfJkpc"
   },
   "source": [
    "Semillas consistentes para todas las funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9AFIAq0Q0VwG"
   },
   "outputs": [],
   "source": [
    "seed_constant = 27\n",
    "np.random.seed(seed_constant)\n",
    "random.seed(seed_constant)\n",
    "tf.random.set_seed(seed_constant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mxsl74v8Ud2i"
   },
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">1. Preprocesamiento de Información</font>**\n",
    "\n",
    "### **<font style=\"color:rgb(134,19,348)\">1.1 Asignación de variables</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gb4Fv7Ag-kwb"
   },
   "outputs": [],
   "source": [
    "# Ancho y alto de los videos del conjunto de datos\n",
    "IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64\n",
    "\n",
    "# Lista de acciones a detectar\n",
    "CLASSES_LIST = ['Buenos dias', 'Buenas tardes', 'Amigo', 'Lo siento', 'Casa', 'z']\n",
    "\n",
    "# Cantidad de videos por acción\n",
    "NO_SEQUENCE = 5\n",
    "\n",
    "# Cantidad de frames por video\n",
    "SEQUENCE_LENGTH = 30\n",
    "\n",
    "# Dirección de los datos\n",
    "DATASET_DIR = \"D:/Work/Aplicada CNN+RNN\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\">1.2. Identificacion de Keypoints usando Mediapipe Holistic</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Inversion de color de BGR a RGB\n",
    "    image.flags.writeable = False                  # Quitar propiedad de escritura\n",
    "    results = model.process(image)                 # Prediccion\n",
    "    image.flags.writeable = True                   # Agregar propiedad de escritura\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # Inversion de color de RGB a BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION) # Dibujar puntos en el rostro\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Puntos de postura\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Puntos mano izquierda\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Puntos mano derecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Colocar puntos en el rostro\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Colocar puntos de postura\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Colocar puntos mano izquierda\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Colocar puntos mano derecha \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\">1.3. Obtención de datos</font>**\n",
    "Creación del dataset a partir de frames tomados con la cámara del dispositivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "zBu224OG-szz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando datos del signo: Buenos dias\n",
      "Generando datos del signo: Buenas tardes\n",
      "Generando datos del signo: Amigo\n",
      "Generando datos del signo: Lo siento\n",
      "Generando datos del signo: Casa\n",
      "Generando datos del signo: z\n",
      "Dataset Finalizado\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cv2.waitKey(1000)\n",
    "#Llamando al modelo mediapipe\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    # Contenedor de los frames extraidos\n",
    "    features = []\n",
    "    # Etiquetas de cada video los videos asociados a cada signo\n",
    "    labels = []\n",
    "    for num, signo in enumerate(CLASSES_LIST): # Para cada signo de la lista\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATASET_DIR, signo))\n",
    "        except:\n",
    "            pass\n",
    "        print(f'Generando datos del signo: {signo}')\n",
    "        \n",
    "        for secuencia in range(NO_SEQUENCE): # Para cada video de cada signo\n",
    "            # Nombre del archivo de video\n",
    "            video_filename = os.path.join(DATASET_DIR, signo, str(secuencia) + '.avi')\n",
    "            # Define el codec y crea el VideoWriter objeto\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "            out = cv2.VideoWriter(video_filename, fourcc, 20.0, (640, 480))\n",
    "            frames_list = []\n",
    "            for n_frame in range(SEQUENCE_LENGTH): #Para cada frame del video\n",
    "                ret, frame=cap.read()\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "                draw_styled_landmarks(image, results)\n",
    "                out.write(image)\n",
    "                if n_frame == 0:\n",
    "                    cv2.putText(image, 'Empezando Recolección', (120,200), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Recolectando frames para {}: \\n Video Numero: {}'.format(signo, secuencia), (15,12),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Mostrar en pantalla\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    cv2.waitKey(1000)\n",
    "                else:\n",
    "                    cv2.putText(image, 'Recolectando frames para {}: \\n Video Numero: {}'.format(signo, secuencia), (15,12),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Mostrar en pantalla\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                resized_frame = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))  # Redimencionamiento de cada frame\n",
    "                normalized_frame = resized_frame / 255                          # Normalizacion de cada frame para dar valores de 0 y 1 a cada pixel\n",
    "                frames_list.append(normalized_frame)\n",
    "\n",
    "            out.release()\n",
    "            features.append(frames_list)\n",
    "            labels.append(num)\n",
    "\n",
    "        \n",
    "    features = np.asarray(features)    \n",
    "    labels=np.array(labels)\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "print('Dataset Finalizado')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\">1.4. Guardar datos de Labels y Features</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperar Datos previos\n",
    "try:\n",
    "    l = np.load('Labels.npy')\n",
    "    f = np.load('Features.nyp')\n",
    "    new_labels = np.concatenate((l,labels))\n",
    "    new_features =np.concatenate((f,features))\n",
    "    np.save('Labels', new_labels)\n",
    "    np.save('Features', new_features)\n",
    "except:\n",
    "    np.save('Labels', labels)\n",
    "    np.save('Features', features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">2. Entrenamiento del modelo</font>**\n",
    "\n",
    "### **<font style=\"color:rgb(134,19,348)\">2.1. Recuperar datos de Labels y Features</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.load('Labels.npy')\n",
    "features = np.load('Features.nyp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "goRGrc8_0Usb"
   },
   "source": [
    "Ahora convertimos los labes obtenidos en vectores one-hot (vectores binarios)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "KfcpuV6_Bd_T"
   },
   "outputs": [],
   "source": [
    "# Se usa el métod de la librería Keras para conertir los labels en vectores one-hot\n",
    "one_hot_encoded_labels = to_categorical(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PzRt7pP-0j5V"
   },
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\">2.2. Separación de datos de entrenamiento</font>**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "UhkQjq1JJSO2"
   },
   "outputs": [],
   "source": [
    "# Dividir la colección de datos en Entrenamiento ( 75% ) y Verificación ( 25% ).\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, one_hot_encoded_labels,\n",
    "                                                                            test_size = 0.25, shuffle = True,\n",
    "                                                                            random_state = seed_constant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8Ui5hLP1P9Q"
   },
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">3. Modelo ConvLSTM</font>**\n",
    "\n",
    "### **<font style=\"color:rgb(134,19,348)\">3.1. Construir el modelo ConvLSTM</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "h8cDSpfXJXwx"
   },
   "outputs": [],
   "source": [
    "def create_convlstm_model():\n",
    "    '''\n",
    "    This function will construct the required convlstm model.\n",
    "    Returns:\n",
    "        model: It is the required constructed convlstm model.\n",
    "    '''\n",
    "\n",
    "    # We will use a Sequential model for model construction\n",
    "    model = Sequential()\n",
    "\n",
    "    # Define the Model Architecture.\n",
    "    ########################################################################################################################\n",
    "    \n",
    "    model.add(ConvLSTM2D(filters = 4, kernel_size = (3, 3), activation = 'tanh',data_format = \"channels_last\",\n",
    "                         recurrent_dropout=0.2, return_sequences=True, input_shape = (SEQUENCE_LENGTH,\n",
    "                                                                                      IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\n",
    "    \n",
    "    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n",
    "    model.add(TimeDistributed(Dropout(0.2)))\n",
    "    \n",
    "    model.add(ConvLSTM2D(filters = 8, kernel_size = (3, 3), activation = 'tanh', data_format = \"channels_last\",\n",
    "                         recurrent_dropout=0.2, return_sequences=True))\n",
    "    \n",
    "    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n",
    "    model.add(TimeDistributed(Dropout(0.2)))\n",
    "    \n",
    "    model.add(ConvLSTM2D(filters = 14, kernel_size = (3, 3), activation = 'tanh', data_format = \"channels_last\",\n",
    "                         recurrent_dropout=0.2, return_sequences=True))\n",
    "    \n",
    "    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n",
    "    model.add(TimeDistributed(Dropout(0.2)))\n",
    "    \n",
    "    model.add(ConvLSTM2D(filters = 16, kernel_size = (3, 3), activation = 'tanh', data_format = \"channels_last\",\n",
    "                         recurrent_dropout=0.2, return_sequences=True))\n",
    "    \n",
    "    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n",
    "    #model.add(TimeDistributed(Dropout(0.2)))\n",
    "    \n",
    "    model.add(Flatten()) \n",
    "    \n",
    "    model.add(Dense(len(CLASSES_LIST), activation = \"softmax\"))\n",
    "    \n",
    "    ########################################################################################################################\n",
    "     \n",
    "    # Display the models summary.\n",
    "    model.summary()\n",
    "    \n",
    "    # Return the constructed convlstm model.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2D1CS2Z6Jkp5",
    "outputId": "9d7d2e7e-f950-4f5c-c88b-810dc7c92c33"
   },
   "outputs": [],
   "source": [
    "# Construir el modelo ConvLSTM.\n",
    "convlstm_model = create_convlstm_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHKkZvkCOnMU"
   },
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\">3.2. Entrenar el modelo ConvLSTM</font>**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yxx_adaPr8Vf",
    "outputId": "e085819d-1200-4298-f142-072cf573ad40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 242ms/step - accuracy: 0.1817 - loss: 1.8171 - val_accuracy: 0.1688 - val_loss: 1.7904\n",
      "Epoch 2/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 208ms/step - accuracy: 0.1791 - loss: 1.7912 - val_accuracy: 0.1299 - val_loss: 1.7898\n",
      "Epoch 3/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 212ms/step - accuracy: 0.2287 - loss: 1.7886 - val_accuracy: 0.1818 - val_loss: 1.7793\n",
      "Epoch 4/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 207ms/step - accuracy: 0.2179 - loss: 1.7726 - val_accuracy: 0.1688 - val_loss: 1.7720\n",
      "Epoch 5/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 491ms/step - accuracy: 0.3101 - loss: 1.6353 - val_accuracy: 0.2338 - val_loss: 1.5760\n",
      "Epoch 6/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 217ms/step - accuracy: 0.4905 - loss: 1.2768 - val_accuracy: 0.3766 - val_loss: 1.7209\n",
      "Epoch 7/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 499ms/step - accuracy: 0.6347 - loss: 1.0616 - val_accuracy: 0.5584 - val_loss: 1.2181\n",
      "Epoch 8/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 447ms/step - accuracy: 0.6625 - loss: 0.8527 - val_accuracy: 0.5714 - val_loss: 1.0372\n",
      "Epoch 9/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 581ms/step - accuracy: 0.7534 - loss: 0.6146 - val_accuracy: 0.6883 - val_loss: 0.8758\n",
      "Epoch 10/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 322ms/step - accuracy: 0.8222 - loss: 0.5292 - val_accuracy: 0.6104 - val_loss: 0.9073\n",
      "Epoch 11/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 214ms/step - accuracy: 0.8498 - loss: 0.4338 - val_accuracy: 0.6234 - val_loss: 1.0573\n",
      "Epoch 12/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 206ms/step - accuracy: 0.9115 - loss: 0.2601 - val_accuracy: 0.6364 - val_loss: 1.1233\n",
      "Epoch 13/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 206ms/step - accuracy: 0.8549 - loss: 0.3511 - val_accuracy: 0.6623 - val_loss: 1.1042\n",
      "Epoch 14/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 207ms/step - accuracy: 0.9153 - loss: 0.2574 - val_accuracy: 0.7403 - val_loss: 0.9003\n",
      "Epoch 15/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 205ms/step - accuracy: 0.9414 - loss: 0.1768 - val_accuracy: 0.6883 - val_loss: 0.8437\n",
      "Epoch 16/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 236ms/step - accuracy: 0.9618 - loss: 0.1166 - val_accuracy: 0.7273 - val_loss: 0.9142\n",
      "Epoch 17/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 588ms/step - accuracy: 0.9482 - loss: 0.1328 - val_accuracy: 0.7532 - val_loss: 0.7870\n",
      "Epoch 18/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 219ms/step - accuracy: 0.9702 - loss: 0.1197 - val_accuracy: 0.7662 - val_loss: 0.9902\n",
      "Epoch 19/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 214ms/step - accuracy: 0.9797 - loss: 0.0720 - val_accuracy: 0.7792 - val_loss: 1.2733\n",
      "Epoch 20/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 227ms/step - accuracy: 0.9533 - loss: 0.0821 - val_accuracy: 0.7792 - val_loss: 0.9736\n",
      "Epoch 21/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 294ms/step - accuracy: 0.9602 - loss: 0.1088 - val_accuracy: 0.7273 - val_loss: 1.0208\n",
      "Epoch 22/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 213ms/step - accuracy: 0.9356 - loss: 0.1652 - val_accuracy: 0.7662 - val_loss: 0.8366\n",
      "Epoch 23/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 208ms/step - accuracy: 0.9563 - loss: 0.0878 - val_accuracy: 0.7532 - val_loss: 0.8150\n",
      "Epoch 24/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 343ms/step - accuracy: 0.9738 - loss: 0.0783 - val_accuracy: 0.7532 - val_loss: 1.0437\n",
      "Epoch 25/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 243ms/step - accuracy: 0.9799 - loss: 0.0619 - val_accuracy: 0.7273 - val_loss: 1.1268\n",
      "Epoch 26/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 220ms/step - accuracy: 0.9870 - loss: 0.0328 - val_accuracy: 0.7403 - val_loss: 1.0389\n",
      "Epoch 27/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 297ms/step - accuracy: 0.9860 - loss: 0.0264 - val_accuracy: 0.7662 - val_loss: 0.9783\n"
     ]
    }
   ],
   "source": [
    "# Detención automática en overfitting\n",
    "early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 10, mode = 'min', restore_best_weights = True)\n",
    "\n",
    "# Función de perdida, optimizador y metricas de evaluación del modelo\n",
    "convlstm_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = [\"accuracy\"])\n",
    "\n",
    "# Inicio del entrenamiento del modelo\n",
    "convlstm_model_training_history = convlstm_model.fit(x = features_train, y = labels_train, epochs = 50, batch_size = 4,\n",
    "                                                     shuffle = True, validation_split = 0.2, \n",
    "                                                     callbacks = [early_stopping_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1KR_kfz2oFl"
   },
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\">3.3. Evaluar el modelo entrenado</font>**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "74tGjokkmSHR",
    "outputId": "f4a69c15-dab9-481b-85a2-fe8c1b3460c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 282ms/step - accuracy: 0.7969 - loss: 0.5756\n"
     ]
    }
   ],
   "source": [
    "# Evaluación del modelo CONVLSTM\n",
    "model_evaluation_history = convlstm_model.evaluate(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric(model_training_history, metric_name_1, metric_name_2, plot_name):\n",
    "    '''\n",
    "    This function will plot the metrics passed to it in a graph.\n",
    "    Args:\n",
    "        model_training_history: A history object containing a record of training and validation \n",
    "                                loss values and metrics values at successive epochs\n",
    "        metric_name_1:          The name of the first metric that needs to be plotted in the graph.\n",
    "        metric_name_2:          The name of the second metric that needs to be plotted in the graph.\n",
    "        plot_name:              The title of the graph.\n",
    "    '''\n",
    "    \n",
    "    # Get metric values using metric names as identifiers.\n",
    "    metric_value_1 = model_training_history.history[metric_name_1]\n",
    "    metric_value_2 = model_training_history.history[metric_name_2]\n",
    "    \n",
    "    # Construct a range object which will be used as x-axis (horizontal plane) of the graph.\n",
    "    epochs = range(len(metric_value_1))\n",
    "\n",
    "    # Plot the Graph.\n",
    "    plt.plot(epochs, metric_value_1, 'blue', label = metric_name_1)\n",
    "    plt.plot(epochs, metric_value_2, 'red', label = metric_name_2)\n",
    "\n",
    "    # Add title to the plot.\n",
    "    plt.title(str(plot_name))\n",
    "\n",
    "    # Add legend to the plot.\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(convlstm_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(convlstm_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGglb-k867iK"
   },
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\">3.4. Guardar el modelo</font>**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "GLfnP_czYudN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Guardar el modelo\n",
    "convlstm_model.save('modelo_convlstm.h5')\n",
    "# Guardar los pesos del modelo\n",
    "convlstm_model.save_weights('convlstm-pesos.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clp7BFVDBNxp"
   },
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">4. Modelo LRCN</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pyKaiYrQ4iM6"
   },
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\">4.1. Construcción del modelo</font>**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "HKM766mB1fIv"
   },
   "outputs": [],
   "source": [
    "def create_LRCN_model():\n",
    "    '''\n",
    "    This function will construct the required LRCN model.\n",
    "    Returns:\n",
    "        model: It is the required constructed LRCN model.\n",
    "    '''\n",
    "\n",
    "    # We will use a Sequential model for model construction.\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Define the Model Architecture.\n",
    "    ########################################################################################################################\n",
    "    \n",
    "    model.add(TimeDistributed(Conv2D(16, (3, 3), padding='same',activation = 'relu'),\n",
    "                              input_shape = (SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\n",
    "    \n",
    "    model.add(TimeDistributed(MaxPooling2D((4, 4)))) \n",
    "    model.add(TimeDistributed(Dropout(0.25)))\n",
    "    \n",
    "    model.add(TimeDistributed(Conv2D(32, (3, 3), padding='same',activation = 'relu')))\n",
    "    model.add(TimeDistributed(MaxPooling2D((4, 4))))\n",
    "    model.add(TimeDistributed(Dropout(0.25)))\n",
    "    \n",
    "    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',activation = 'relu')))\n",
    "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "    model.add(TimeDistributed(Dropout(0.25)))\n",
    "    \n",
    "    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',activation = 'relu')))\n",
    "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "    #model.add(TimeDistributed(Dropout(0.25)))\n",
    "                                      \n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "                                      \n",
    "    model.add(LSTM(32))\n",
    "                                      \n",
    "    model.add(Dense(len(CLASSES_LIST), activation = 'softmax'))\n",
    "\n",
    "    ########################################################################################################################\n",
    "\n",
    "    # Display the models summary.\n",
    "    model.summary()\n",
    "    \n",
    "    # Return the constructed LRCN model.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZJjC2VA2JkqD",
    "outputId": "986da870-0d0c-4469-ffca-d538635f75e9"
   },
   "outputs": [],
   "source": [
    "\n",
    "LRCN_model = create_LRCN_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsDrzxCDA5lX"
   },
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\">4.2. Entrenar el modelo</font>**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mtrQGXc8A5lY",
    "outputId": "a11b7a40-9198-4875-e531-a8900e96c636"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 163ms/step - accuracy: 0.1517 - loss: 1.8254 - val_accuracy: 0.1948 - val_loss: 1.7902\n",
      "Epoch 2/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 163ms/step - accuracy: 0.1479 - loss: 1.7908 - val_accuracy: 0.2987 - val_loss: 1.7883\n",
      "Epoch 3/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 166ms/step - accuracy: 0.1399 - loss: 1.7818 - val_accuracy: 0.3766 - val_loss: 1.7330\n",
      "Epoch 4/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 200ms/step - accuracy: 0.2187 - loss: 1.6725 - val_accuracy: 0.4286 - val_loss: 1.3296\n",
      "Epoch 5/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 156ms/step - accuracy: 0.4470 - loss: 1.3300 - val_accuracy: 0.6623 - val_loss: 0.9188\n",
      "Epoch 6/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 126ms/step - accuracy: 0.6724 - loss: 0.9996 - val_accuracy: 0.7662 - val_loss: 0.6956\n",
      "Epoch 7/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 589ms/step - accuracy: 0.7574 - loss: 0.7553 - val_accuracy: 0.7662 - val_loss: 0.5781\n",
      "Epoch 8/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 729ms/step - accuracy: 0.8095 - loss: 0.5398 - val_accuracy: 0.8182 - val_loss: 0.4691\n",
      "Epoch 9/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 630ms/step - accuracy: 0.8464 - loss: 0.4126 - val_accuracy: 0.9091 - val_loss: 0.4333\n",
      "Epoch 10/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 547ms/step - accuracy: 0.9280 - loss: 0.2778 - val_accuracy: 0.9221 - val_loss: 0.3708\n",
      "Epoch 11/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 504ms/step - accuracy: 0.9369 - loss: 0.2492 - val_accuracy: 0.8701 - val_loss: 0.4305\n",
      "Epoch 12/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 495ms/step - accuracy: 0.9171 - loss: 0.2708 - val_accuracy: 0.8961 - val_loss: 0.3921\n",
      "Epoch 13/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 502ms/step - accuracy: 0.9320 - loss: 0.2441 - val_accuracy: 0.9351 - val_loss: 0.2811\n",
      "Epoch 14/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 507ms/step - accuracy: 0.9605 - loss: 0.1449 - val_accuracy: 0.8961 - val_loss: 0.3103\n",
      "Epoch 15/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 443ms/step - accuracy: 0.9799 - loss: 0.0983 - val_accuracy: 0.9481 - val_loss: 0.2651\n",
      "Epoch 16/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 224ms/step - accuracy: 0.9795 - loss: 0.0796 - val_accuracy: 0.9351 - val_loss: 0.2736\n",
      "Epoch 17/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 568ms/step - accuracy: 0.9876 - loss: 0.0459 - val_accuracy: 0.9351 - val_loss: 0.2674\n",
      "Epoch 18/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 308ms/step - accuracy: 0.9925 - loss: 0.0314 - val_accuracy: 0.9481 - val_loss: 0.2107\n",
      "Epoch 19/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 326ms/step - accuracy: 1.0000 - loss: 0.0214 - val_accuracy: 0.9221 - val_loss: 0.2359\n",
      "Epoch 20/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 208ms/step - accuracy: 0.9765 - loss: 0.0844 - val_accuracy: 0.9091 - val_loss: 0.3247\n",
      "Epoch 21/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 136ms/step - accuracy: 0.9358 - loss: 0.1446 - val_accuracy: 0.9221 - val_loss: 0.2866\n",
      "Epoch 22/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 139ms/step - accuracy: 0.9905 - loss: 0.0791 - val_accuracy: 0.9221 - val_loss: 0.2774\n",
      "Epoch 23/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 369ms/step - accuracy: 1.0000 - loss: 0.0166 - val_accuracy: 0.9221 - val_loss: 0.2884\n",
      "Epoch 24/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 502ms/step - accuracy: 1.0000 - loss: 0.0155 - val_accuracy: 0.9091 - val_loss: 0.3414\n",
      "Epoch 25/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 428ms/step - accuracy: 0.9839 - loss: 0.0754 - val_accuracy: 0.9221 - val_loss: 0.2963\n",
      "Epoch 26/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 237ms/step - accuracy: 0.9969 - loss: 0.0209 - val_accuracy: 0.9481 - val_loss: 0.2485\n",
      "Epoch 27/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 406ms/step - accuracy: 1.0000 - loss: 0.0121 - val_accuracy: 0.9481 - val_loss: 0.2922\n",
      "Epoch 28/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 607ms/step - accuracy: 1.0000 - loss: 0.0061 - val_accuracy: 0.9481 - val_loss: 0.2976\n",
      "Epoch 29/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 291ms/step - accuracy: 1.0000 - loss: 0.0057 - val_accuracy: 0.9091 - val_loss: 0.3288\n",
      "Epoch 30/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 133ms/step - accuracy: 0.9542 - loss: 0.1550 - val_accuracy: 0.8831 - val_loss: 0.3509\n",
      "Epoch 31/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 133ms/step - accuracy: 0.9658 - loss: 0.1234 - val_accuracy: 0.9351 - val_loss: 0.2974\n",
      "Epoch 32/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 131ms/step - accuracy: 0.9877 - loss: 0.0637 - val_accuracy: 0.9481 - val_loss: 0.2338\n",
      "Epoch 33/50\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 191ms/step - accuracy: 0.9971 - loss: 0.0235 - val_accuracy: 0.9091 - val_loss: 0.3392\n"
     ]
    }
   ],
   "source": [
    "# Create an Instance of Early Stopping Callback.\n",
    "early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 15, mode = 'min', restore_best_weights = True)\n",
    " \n",
    "# Compile the model and specify loss function, optimizer and metrics to the model.\n",
    "LRCN_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = [\"accuracy\"])\n",
    "\n",
    "# Start training the model.\n",
    "LRCN_model_training_history = LRCN_model.fit(x = features_train, y = labels_train, epochs = 50, batch_size = 4 ,\n",
    "                                             shuffle = True, validation_split = 0.2, callbacks = [early_stopping_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMepQus5A5la"
   },
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\">4.3. Evaluating the trained Model</font>**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZwSYdhEFA5lb",
    "outputId": "fc0d73b3-128a-4ed1-d04a-f66768d90488"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9667 - loss: 0.1511\n"
     ]
    }
   ],
   "source": [
    "# Evaluación del modelo LRCN\n",
    "model_evaluation_history = LRCN_model.evaluate(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valores de Perdida durante el entrenamiento\n",
    "plot_metric(LRCN_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valores de Exactitud durante el entrenamiento\n",
    "plot_metric(LRCN_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SNLhkFNA5lb"
   },
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\">4.4. Guardar el modelo</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LRCN_model.save_weights('LRCN-pesos.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">5. Probar el modelo en tiempo real</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Setear modelo mediapipe \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    exit_flag = True\n",
    "    while cap.isOpened() and exit_flag:\n",
    "        frames_list = []\n",
    "        for n_frame in range(SEQUENCE_LENGTH): #Para cada frame del video\n",
    "            ret, frame=cap.read()\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "            draw_styled_landmarks(image, results)\n",
    "            resized_frame = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))  # Redimencionamiento de cada frame\n",
    "            normalized_frame = resized_frame / 255                          # Normalizacion de cada frame para dar valores de 0 y 1 a cada pixel\n",
    "            frames_list.append(normalized_frame)\n",
    "\n",
    "            if len(frames_list) == SEQUENCE_LENGTH:\n",
    "                # # Pass the normalized frames to the model and get the predicted probabilities.\n",
    "                predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_list, axis = 0))[0]\n",
    "\n",
    "                # # Get the index of class with highest probability.qq\n",
    "                predicted_label = np.argmax(predicted_labels_probabilities)\n",
    "\n",
    "                # # Get the class name using the retrieved index.\n",
    "                predicted_class_name = CLASSES_LIST[predicted_label]\n",
    "\n",
    "                # Visualizacion\n",
    "                cv2.rectangle(image, (0,0), (250, 40), (245, 117, 16), -1)\n",
    "                cv2.putText(image, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "                # Mostrar en pantalla\n",
    "                cv2.imshow('OpenCV Feed', image)\n",
    "                cv2.waitKey(1000)\n",
    "\n",
    "            else:\n",
    "                cv2.putText(image, 'Recolectando frames....', (15,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                # Mostrar en pantalla\n",
    "                cv2.imshow('OpenCV Feed', image)\n",
    "                \n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                exit_flag = False\n",
    "                break\n",
    "        \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Human Action Recogntion using CNN + LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
