{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VuI3Z8l1Ps8a"
   },
   "source": [
    "\n",
    "# **<center><font style=\"color:rgb(100,109,254)\">Reconocimiento de señas en tiempo real (CNN + LSTM)</font> </center>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RiAMJx7tr5-I"
   },
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\"> Librerías e importaciones</font>**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ft-u2b1iUQpg"
   },
   "outputs": [],
   "source": [
    "# Instalaciones\n",
    "!pip install tensorflow opencv-contrib-python youtube-dl moviepy pydot graphviz\n",
    "!pip install git+https://github.com/TahaAnwar/pafy.git#egg=pafy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "X3AdbpZFCRR0"
   },
   "outputs": [],
   "source": [
    "# Importaciones\n",
    "import os\n",
    "import cv2\n",
    "import pafy\n",
    "import math\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "\n",
    "from moviepy.editor import *\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mxsl74v8Ud2i"
   },
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Preprocesamiento de datos de Entrada</font>**\n",
    "\n",
    "Inicialización de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "gb4Fv7Ag-kwb"
   },
   "outputs": [],
   "source": [
    "# Specify the height and width to which each video frame will be resized in our dataset.\n",
    "IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64\n",
    "\n",
    "# Lista de acciones a detectar\n",
    "CLASSES_LIST = ['Buenos dias', 'Buenas tardes', 'Amigo', 'Lo siento', 'Casa', 'z']\n",
    "\n",
    "# Cantidad de videos por acción\n",
    "NO_SEQUENCE = 30\n",
    "\n",
    "# Cantidad de frames por video\n",
    "SEQUENCE_LENGTH = 30\n",
    "\n",
    "# Dirección de los datos. \n",
    "DATASET_DIR = \"D:/Work/Aplicada CNN+RNN\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Identificacion de Keypoints usando Mediapipe </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Inversion de color de BGR a RGB\n",
    "    image.flags.writeable = False                  # Quitar propiedad de escritura\n",
    "    results = model.process(image)                 # Prediccion\n",
    "    image.flags.writeable = True                   # Agregar propiedad de escritura\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # Inversion de color de RGB a BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION) # Dibujar puntos en el rostro\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Puntos de postura\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Puntos mano izquierda\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Puntos mano derecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Colocar puntos en el rostro\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Colocar puntos de postura\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Colocar puntos mano izquierda\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Colocar puntos mano derecha \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zx7PoAS7iEHM"
   },
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">1. Modelo ConvLSTM</font>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\">1.1. Construcción de modelo ConvLSTM</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "h8cDSpfXJXwx"
   },
   "outputs": [],
   "source": [
    "def create_convlstm_model():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Modelo de arquitectura.\n",
    "    ########################################################################################################################\n",
    "    \n",
    "    model.add(ConvLSTM2D(filters = 4, kernel_size = (3, 3), activation = 'tanh',data_format = \"channels_last\",\n",
    "                         recurrent_dropout=0.2, return_sequences=True, input_shape = (SEQUENCE_LENGTH,\n",
    "                                                                                      IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\n",
    "    \n",
    "    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n",
    "    model.add(TimeDistributed(Dropout(0.2)))\n",
    "    \n",
    "    model.add(ConvLSTM2D(filters = 8, kernel_size = (3, 3), activation = 'tanh', data_format = \"channels_last\",\n",
    "                         recurrent_dropout=0.2, return_sequences=True))\n",
    "    \n",
    "    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n",
    "    model.add(TimeDistributed(Dropout(0.2)))\n",
    "    \n",
    "    model.add(ConvLSTM2D(filters = 14, kernel_size = (3, 3), activation = 'tanh', data_format = \"channels_last\",\n",
    "                         recurrent_dropout=0.2, return_sequences=True))\n",
    "    \n",
    "    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n",
    "    model.add(TimeDistributed(Dropout(0.2)))\n",
    "    \n",
    "    model.add(ConvLSTM2D(filters = 16, kernel_size = (3, 3), activation = 'tanh', data_format = \"channels_last\",\n",
    "                         recurrent_dropout=0.2, return_sequences=True))\n",
    "    \n",
    "    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n",
    "    #model.add(TimeDistributed(Dropout(0.2)))\n",
    "    \n",
    "    model.add(Flatten()) \n",
    "    \n",
    "    model.add(Dense(len(CLASSES_LIST), activation = \"softmax\"))\n",
    "    \n",
    "    ########################################################################################################################\n",
    "     \n",
    "    # Summary del modelo.\n",
    "    model.summary()\n",
    "    \n",
    "    # Retorno del modelo.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\">1.2. Recuperar el modelo ConvLSTM</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convlstm_model = create_convlstm_model()\n",
    "convlstm_model.load_weights('modelo_convlstm.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clp7BFVDBNxp"
   },
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">2. Modelo LRCN</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pyKaiYrQ4iM6"
   },
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\">2.1. Construir el modelo</font>**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "HKM766mB1fIv"
   },
   "outputs": [],
   "source": [
    "def create_LRCN_model():\n",
    "    '''\n",
    "    This function will construct the required LRCN model.\n",
    "    Returns:\n",
    "        model: It is the required constructed LRCN model.\n",
    "    '''\n",
    "\n",
    "    # We will use a Sequential model for model construction.\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Define the Model Architecture.\n",
    "    ########################################################################################################################\n",
    "    \n",
    "    model.add(TimeDistributed(Conv2D(16, (3, 3), padding='same',activation = 'relu'),\n",
    "                              input_shape = (SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\n",
    "    \n",
    "    model.add(TimeDistributed(MaxPooling2D((4, 4)))) \n",
    "    model.add(TimeDistributed(Dropout(0.25)))\n",
    "    \n",
    "    model.add(TimeDistributed(Conv2D(32, (3, 3), padding='same',activation = 'relu')))\n",
    "    model.add(TimeDistributed(MaxPooling2D((4, 4))))\n",
    "    model.add(TimeDistributed(Dropout(0.25)))\n",
    "    \n",
    "    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',activation = 'relu')))\n",
    "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "    model.add(TimeDistributed(Dropout(0.25)))\n",
    "    \n",
    "    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',activation = 'relu')))\n",
    "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "    #model.add(TimeDistributed(Dropout(0.25)))\n",
    "                                      \n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "                                      \n",
    "    model.add(LSTM(32))\n",
    "                                      \n",
    "    model.add(Dense(len(CLASSES_LIST), activation = 'softmax'))\n",
    "\n",
    "    ########################################################################################################################\n",
    "\n",
    "    # Display the models summary.\n",
    "    model.summary()\n",
    "    \n",
    "    # Return the constructed LRCN model.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SNLhkFNA5lb"
   },
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\">2.2. Recuperar el modelo LRCN</font>**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construir el modelo\n",
    "LRCN_model = create_LRCN_model()\n",
    "\n",
    "# Recuperar los pesos\n",
    "LRCN_model.load_weights('LRCN-pesos.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">3. Reconocimiento en tiempo real</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Setear modelo mediapipe \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    exit_flag = True\n",
    "    while cap.isOpened() and exit_flag:\n",
    "        frames_list = []\n",
    "        for n_frame in range(SEQUENCE_LENGTH): #Para cada frame del video\n",
    "            ret, frame=cap.read()\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "            draw_styled_landmarks(image, results)\n",
    "            resized_frame = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))  # Redimencionamiento de cada frame\n",
    "            normalized_frame = resized_frame / 255                          # Normalizacion de cada frame para dar valores de 0 y 1 a cada pixel\n",
    "            frames_list.append(normalized_frame)\n",
    "\n",
    "            if len(frames_list) == SEQUENCE_LENGTH:\n",
    "                # # Pass the normalized frames to the model and get the predicted probabilities.\n",
    "                predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_list, axis = 0))[0]\n",
    "\n",
    "                # # Get the index of class with highest probability.qq\n",
    "                predicted_label = np.argmax(predicted_labels_probabilities)\n",
    "\n",
    "                # # Get the class name using the retrieved index.\n",
    "                predicted_class_name = CLASSES_LIST[predicted_label]\n",
    "\n",
    "                # Visualizacion\n",
    "                cv2.rectangle(image, (0,0), (250, 40), (245, 117, 16), -1)\n",
    "                cv2.putText(image, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "                # Mostrar en pantalla\n",
    "                cv2.imshow('OpenCV Feed', image)\n",
    "                cv2.waitKey(1000)\n",
    "\n",
    "            else:\n",
    "                cv2.putText(image, 'Recolectando frames....', (15,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                # Mostrar en pantalla\n",
    "                cv2.imshow('OpenCV Feed', image)\n",
    "                \n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                exit_flag = False\n",
    "                break\n",
    "        \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Human Action Recogntion using CNN + LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
